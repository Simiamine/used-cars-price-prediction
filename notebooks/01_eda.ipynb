{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction du prix de véhicules d'occasion (Craigslist) – Régression par réseau de neurones et comparaison d'optimiseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction et contexte du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet vise à prédire le prix de véhicules d'occasion à partir de leurs caractéristiques, en utilisant un réseau de neurones.  \n",
    "Nous utilisons le jeu de données *Craigslist Cars and Trucks* (annonces de voitures d'occasion sur Craigslist) fourni par l'utilisateur.  \n",
    "Ce jeu de données est volumineux (~1,45 Go pour ~1,7 million d'enregistrements) et contient de nombreuses informations sur les véhicules listés (prix, état du véhicule, constructeur, etc.)​.\n",
    "\n",
    "Nous allons exploiter ces données pour entraîner un modèle de régression qui estimera le prix de vente à partir des attributs du véhicule.\n",
    "\n",
    "## L'objectif est double :\n",
    "\n",
    "- Construire un modèle de réseau de neurones simple pour cette tâche de régression (prédiction de prix).\n",
    "- Étudier l'influence de différents algorithmes d'optimisation sur la convergence du modèle et sa performance.  \n",
    "Nous comparerons au minimum **SGD (Stochastic Gradient Descent)** et **Adam**, et potentiellement d'autres optimiseurs adaptatifs comme **RMSprop** ou **Adagrad**.\n",
    "\n",
    "---\n",
    "\n",
    "## Évaluation des performances\n",
    "\n",
    "Nous évaluerons principalement la **MAE (Mean Absolute Error)** comme métrique de performance, car elle donne une idée claire de l'erreur moyenne en valeur monétaire (euros) entre le prix prédit et le prix réel.\n",
    "\n",
    "La **MSE (Mean Squared Error)** sera également observée (notamment en tant que fonction de perte pour l'entraînement) afin de détecter d'éventuelles grandes erreurs (puisque MSE pénalise davantage les écarts élevés).\n",
    "\n",
    "---\n",
    "\n",
    "## Approche expérimentale (étapes)\n",
    "\n",
    "Nous procéderons étape par étape :\n",
    "\n",
    "1. **Préparation et nettoyage des données**  \n",
    "   → filtrage des colonnes inutiles, gestion des valeurs manquantes, encodage des variables qualitatives (catégorielles) et normalisation des variables quantitatives.\n",
    "\n",
    "2. **Définition du modèle**  \n",
    "   → un réseau de neurones à 2-3 couches *fully connected* avec fonctions d'activation **ReLU** et une sortie **linéaire** adaptée à la régression.\n",
    "\n",
    "3. **Entraînement avec différents optimiseurs**  \n",
    "   → nous entraînerons le même modèle avec plusieurs algorithmes d'optimisation (**SGD**, **Adam**, **RMSprop**, **Adagrad**) sur un nombre fixe d'époques (par ex. 100) pour comparer leur vitesse de convergence et leur performance finale.  \n",
    "   → Nous suivrons l'évolution de la **MAE** (et de la loss **MSE**) au fil des époques pour chaque optimiseur.\n",
    "\n",
    "4. **Mesure des temps d'entraînement**  \n",
    "   → pour chaque optimiseur, nous mesurerons le temps nécessaire pour effectuer 100 époques d'entraînement, afin de comparer l'efficacité computationnelle en plus de l'efficacité en termes de convergence.\n",
    "\n",
    "5. **Comparaison des performances**  \n",
    "   → nous superposerons les courbes de perte et de MAE des différents runs pour visualiser clairement les différences de convergence.  \n",
    "   → Nous comparerons également les MAE finales atteintes sur le jeu de validation/test par chaque méthode.\n",
    "\n",
    "6. **Analyse de l'importance des caractéristiques**  \n",
    "   → finalement, nous étudierons l'impact de chaque attribut du jeu de données sur le résultat en supprimant les colonnes une à une et en observant l'augmentation de l'erreur (**MAE**) sans cette information.  \n",
    "   → Cette *analyse par ablation* nous permettra d'identifier quelles caractéristiques sont les plus influentes pour prédire le prix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation et nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons **charger le jeu de données** et le **préparer pour l'entraînement du modèle**.\r\n",
    "\r\n",
    "### Étapes réalisées :\r\n",
    "\r\n",
    "- **Chargement des données brutes** depuis le fichier CSV.\r\n",
    "\r\n",
    "- **Suppression des colonnes non pertinentes** pour notre objectif :  \r\n",
    "  (ex : identifiants, descriptions textuelles, URL d'images, coordonnées GPS, etc.), qui n’apportent pas d’information utile pour la prédiction du prix.\r\n",
    "\r\n",
    "- **Gestion des valeurs manquantes** :  \r\n",
    "  Pour simplifier, on choisit ici de supprimer les lignes incomplètes. Cette approche est acceptable compte tenu de la **taille importante du dataset**.\r\n",
    "\r\n",
    "- **Transformation de la date** (`posting_date`) :  \r\n",
    "  Elle est convertie en format date/heure, puis utilisée pour extraire des **features temporelles** (année, mois, jour de la semaine), utiles pour capturer d’éventuels effets saisonniers ou tendances.\r\n",
    "\r\n",
    "- **Filtrage des outliers** :  \r\n",
    "  On garde uniquement les annonces dont le **prix est réaliste** (par exemple entre **100€ et 250 000€**) pour éviter que des valeurs aberrantes perturbent l’apprentissage du modèle.\r\n",
    "\r\n",
    "- **Séparation des caractéristiques et de la cible** :  \r\n",
    "  La colonne `price` est la **variable cible** à prédire, les autres colonnes seront les **features explicatives**.\r\n",
    "\r\n",
    "- **Encodage des variables catégorielles** :  \r\n",
    "  Les variables comme `manufacturer`, `condition`, `fuel`, `transmission`, etc. sont transformées par **One-Hot Encoding** : chaque catégorie devient une **colonne binaire** (0 ou 1) indiquant sa présence.\r\n",
    "\r\n",
    "- **Normalisation des variables numériques** :  \r\n",
    "  Les colonnes comme `year`, `odometer`, etc. ont des **échelles différentes**.  \r\n",
    "  Elles sont standardisées (**soustraction de la moyenne**, **division par l’écart-type**) pour accélérer et stabiliser l’entraînement.  \r\n",
    "  ⚠️ La cible `price` **n’est pas normalisée**, pour conserver une interprétation directe des erreurs (MAE en euros).\r\n",
    "\r\n",
    "- **Découpage en ensembles d’entraînement, validation et test** :  \r\n",
    "  Typiquement :  \r\n",
    "  - 70% pour l’entraînement  \r\n",
    "  - 15% pour la validation  \r\n",
    "  - 15% pour le test  \r\n",
    "  La **validation** est utilisée pendant l’entraînement pour surveiller la performance sur des données non vues.  \r\n",
    "  La séparation est **aléatoire**, avec un `random_state` fixé pour assurer la reproductibilité.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Exécutons ces étapes **une par une**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du jeu de données et sélection des colonnes utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "Dans le dataset original, les colonnes disponibles incluent (entre autres) :\r\n",
    "\r\n",
    "`region`, `price`, `year`, `manufacturer`, `model`, `condition`, `cylinders`, `fuel`,  \r\n",
    "`odometer`, `title_status`, `transmission`, `drive`, `size`, `type`, `lat`, `long`,  \r\n",
    "`posting_date`, `state`, etc.\r\n",
    "\r\n",
    "Toutes **ne sont pas pertinentes** pour prédire le prix. Voici pourquoi certaines seront ignorées :\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ❌ Colonnes exclues :\r\n",
    "\r\n",
    "- **`region`** et **`county`** : trop locales ou redondantes avec l'État (`state`).\r\n",
    "- **`lat`** / **`long`** : coordonnées GPS trop précises, peu utiles pour une estimation globale.\r\n",
    "- **`VIN`** : identifiant unique sans utilité dans une modélisation globale.\r\n",
    "- **`url`, `region_url`, `image_url`** : liens d'annonce, inutiles ici.\r\n",
    "- **`description`** : texte libre non structuré que nous ne traiterons pas ici.\r\n",
    "- **`id`** : identifiant unique de l’annonce.\r\n",
    "- **`model`** : bien qu’informatif, il y a **trop de valeurs distinctes** (ex : \"Corolla\", \"F-150\"...).  \r\n",
    "  Pour cette première version, nous le laissons de côté pour éviter une complexité excessive.  \r\n",
    "  La **marque** (`manufacturer`) suffira à capter l’effet du modèle.\r\n",
    "- **`size`** : remplacée implicitement par `type` (berline, SUV, etc.).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ✅ Colonnes conservées :\r\n",
    "\r\n",
    "- **`price`** : cible à prédire.\r\n",
    "- **`year`** : année de mise en circulation du véhicule.\r\n",
    "- **`manufacturer`** : marque du véhicule (Toyota, Ford…).\r\n",
    "- **`condition`** : état du véhicule (neuf, excellent, bon…).\r\n",
    "- **`cylinders`** : nombre de cylindres (catégoriel dans le dataset).\r\n",
    "- **`fuel`** : type de carburant (essence, diesel, électrique…).\r\n",
    "- **`odometer`** : kilométrage.\r\n",
    "- **`transmission`** : type de boîte de vitesses (manuelle, automatique…).\r\n",
    "- **`drive`** : type de traction (propulsion, traction, 4x4…).\r\n",
    "- **`type`** : type de véhicule (pickup, berline, SUV…).\r\n",
    "- **`paint_color`** : couleur extérieure.\r\n",
    "- **`state`** : code de l’État (USA).\r\n",
    "- **`title_status`** : état administratif du véhicule (clean, rebuilt, salvage…).\r\n",
    "- **`posting_date`** : date de publication de l’annonce  \r\n",
    "  ➤ On l’utilisera pour **extraire des informations temporelles** (année, mois, jour).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "🔧 **Nous allons maintenant procéder au chargement du dataset et au filtrage initial des colonnes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions initiales du dataset : (426880, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>region</th>\n",
       "      <th>region_url</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>condition</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>...</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "      <th>paint_color</th>\n",
       "      <th>image_url</th>\n",
       "      <th>description</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>posting_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7222695916</td>\n",
       "      <td>https://prescott.craigslist.org/cto/d/prescott...</td>\n",
       "      <td>prescott</td>\n",
       "      <td>https://prescott.craigslist.org</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>az</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7218891961</td>\n",
       "      <td>https://fayar.craigslist.org/ctd/d/bentonville...</td>\n",
       "      <td>fayetteville</td>\n",
       "      <td>https://fayar.craigslist.org</td>\n",
       "      <td>11900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7221797935</td>\n",
       "      <td>https://keys.craigslist.org/cto/d/summerland-k...</td>\n",
       "      <td>florida keys</td>\n",
       "      <td>https://keys.craigslist.org</td>\n",
       "      <td>21000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7222270760</td>\n",
       "      <td>https://worcester.craigslist.org/cto/d/west-br...</td>\n",
       "      <td>worcester / central MA</td>\n",
       "      <td>https://worcester.craigslist.org</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7210384030</td>\n",
       "      <td>https://greensboro.craigslist.org/cto/d/trinit...</td>\n",
       "      <td>greensboro</td>\n",
       "      <td>https://greensboro.craigslist.org</td>\n",
       "      <td>4900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                                url  \\\n",
       "0  7222695916  https://prescott.craigslist.org/cto/d/prescott...   \n",
       "1  7218891961  https://fayar.craigslist.org/ctd/d/bentonville...   \n",
       "2  7221797935  https://keys.craigslist.org/cto/d/summerland-k...   \n",
       "3  7222270760  https://worcester.craigslist.org/cto/d/west-br...   \n",
       "4  7210384030  https://greensboro.craigslist.org/cto/d/trinit...   \n",
       "\n",
       "                   region                         region_url  price  year  \\\n",
       "0                prescott    https://prescott.craigslist.org   6000   NaN   \n",
       "1            fayetteville       https://fayar.craigslist.org  11900   NaN   \n",
       "2            florida keys        https://keys.craigslist.org  21000   NaN   \n",
       "3  worcester / central MA   https://worcester.craigslist.org   1500   NaN   \n",
       "4              greensboro  https://greensboro.craigslist.org   4900   NaN   \n",
       "\n",
       "  manufacturer model condition cylinders  ... size  type paint_color  \\\n",
       "0          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "1          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "2          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "3          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "4          NaN   NaN       NaN       NaN  ...  NaN   NaN         NaN   \n",
       "\n",
       "  image_url description county state lat long posting_date  \n",
       "0       NaN         NaN    NaN    az NaN  NaN          NaN  \n",
       "1       NaN         NaN    NaN    ar NaN  NaN          NaN  \n",
       "2       NaN         NaN    NaN    fl NaN  NaN          NaN  \n",
       "3       NaN         NaN    NaN    ma NaN  NaN          NaN  \n",
       "4       NaN         NaN    NaN    nc NaN  NaN          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données depuis le fichier CSV\n",
    "import pandas as pd\n",
    "\n",
    "# Lire le fichier CSV (en supposant qu'il est dans le répertoire courant ou préciser le chemin)\n",
    "df = pd.read_csv(\"../data/vehicles.csv\")\n",
    "\n",
    "# Afficher le nombre de lignes/colonnes initial et un aperçu des premières lignes\n",
    "print(\"Dimensions initiales du dataset :\", df.shape)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions après suppression de colonnes inutiles : (426880, 14)\n",
      "Colonnes restantes : ['price', 'year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'paint_color', 'state', 'posting_date']\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les colonnes non pertinentes pour la prédiction du prix\n",
    "cols_to_drop = ['id', 'url', 'region', 'region_url', 'description', 'model', \n",
    "                'VIN', 'size', 'county', 'lat', 'long', 'image_url']  # liste de colonnes à éliminer\n",
    "df.drop(columns=cols_to_drop, errors='ignore', inplace=True)  # errors='ignore' au cas où certaines colonnes n'existent pas\n",
    "\n",
    "print(\"Dimensions après suppression de colonnes inutiles :\", df.shape)\n",
    "print(\"Colonnes restantes :\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des valeurs manquantes et extraction de la date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons réduit le dataset aux **colonnes d'intérêt**, traitons les **valeurs manquantes**.\r\n",
    "\r\n",
    "Il est fréquent d’avoir des données incomplètes dans ce type de jeu de données.  \r\n",
    "Par exemple, un vendeur peut ne pas avoir renseigné :\r\n",
    "\r\n",
    "- le **kilométrage** (`odometer`)\r\n",
    "- l’**état du véhicule** (`condition`)\r\n",
    "- ou d’autres informations importantes\r\n",
    "\r\n",
    "Pour **simplifier l’analyse**, nous allons **supprimer les enregistrements** contenant des valeurs manquantes dans les colonnes retenues.\r\n",
    "\r\n",
    "Cette méthode peut entraîner une **réduction du nombre d’exemples**,  \r\n",
    "mais ce n’est **pas problématique ici**, car le dataset initial est très volumineux.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Transformation de la date `posting_date`\r\n",
    "\r\n",
    "La colonne `posting_date` est à l’origine une chaîne de caractères (`string`), que nous allons :\r\n",
    "\r\n",
    "1. **Convertir au format `datetime`**\r\n",
    "2. Extraire les **informations temporelles** suivantes :\r\n",
    "   - `year_posted` : **année** de publication (ex : 2018, 2019…)\r\n",
    "   - `month_posted` : **mois** de publication (1 à 12)\r\n",
    "   - `weekday_posted` : **jour de la semaine** (0 = lundi, ..., 6 = dimanche)\r\n",
    "\r\n",
    "Ces nouvelles variables peuvent **capter des effets temporels**, comme :\r\n",
    "- des **variations de prix au fil des années**\r\n",
    "- une **différence de comportement** entre les publications en semaine et en week-end\r\n",
    "\r\n",
    "Une fois ces informations extraites, la colonne brute `posting_date` ne sera **plus utile**,  \r\n",
    "elle sera donc **supprimée**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Filtrage des outliers sur `price`\r\n",
    "\r\n",
    "On élimine les annonces dont le **prix est aberrant**, en ne gardant que les véhicules dont le prix est :\r\n",
    "\r\n",
    "- **≥ 100 €**\r\n",
    "- **≤ 250 000 €**\r\n",
    "\r\n",
    "Ces bornes sont arbitraires, mais permettent de :\r\n",
    "- supprimer les annonces à **prix nul ou trop faible**, souvent des **erreurs** ou annonces tests\r\n",
    "- exclure les **véhicules très luxueux** ou **mal saisis**, qui pourraient **fausser l’entraînement**\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Nous allons maintenant **appliquer** ces transformations dans la cellule de code suivante.ransformations dans la cellule de code suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions après suppression des valeurs manquantes : (117169, 14)\n",
      "Dimensions après nettoyage des dates : (117169, 14)\n",
      "Dimensions finales après filtrage des outliers de prix : (111643, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>condition</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>fuel</th>\n",
       "      <th>odometer</th>\n",
       "      <th>title_status</th>\n",
       "      <th>transmission</th>\n",
       "      <th>drive</th>\n",
       "      <th>type</th>\n",
       "      <th>paint_color</th>\n",
       "      <th>state</th>\n",
       "      <th>year_posted</th>\n",
       "      <th>month_posted</th>\n",
       "      <th>weekday_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>ford</td>\n",
       "      <td>excellent</td>\n",
       "      <td>6 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>clean</td>\n",
       "      <td>automatic</td>\n",
       "      <td>rwd</td>\n",
       "      <td>truck</td>\n",
       "      <td>black</td>\n",
       "      <td>al</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27990</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>gmc</td>\n",
       "      <td>good</td>\n",
       "      <td>8 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>68696.0</td>\n",
       "      <td>clean</td>\n",
       "      <td>other</td>\n",
       "      <td>4wd</td>\n",
       "      <td>pickup</td>\n",
       "      <td>black</td>\n",
       "      <td>al</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34590</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>good</td>\n",
       "      <td>6 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>29499.0</td>\n",
       "      <td>clean</td>\n",
       "      <td>other</td>\n",
       "      <td>4wd</td>\n",
       "      <td>pickup</td>\n",
       "      <td>silver</td>\n",
       "      <td>al</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>toyota</td>\n",
       "      <td>excellent</td>\n",
       "      <td>6 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>clean</td>\n",
       "      <td>automatic</td>\n",
       "      <td>4wd</td>\n",
       "      <td>truck</td>\n",
       "      <td>grey</td>\n",
       "      <td>al</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29990</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>good</td>\n",
       "      <td>6 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>17302.0</td>\n",
       "      <td>clean</td>\n",
       "      <td>other</td>\n",
       "      <td>4wd</td>\n",
       "      <td>pickup</td>\n",
       "      <td>red</td>\n",
       "      <td>al</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price    year manufacturer  condition    cylinders fuel  odometer  \\\n",
       "0  15000  2013.0         ford  excellent  6 cylinders  gas  128000.0   \n",
       "1  27990  2012.0          gmc       good  8 cylinders  gas   68696.0   \n",
       "2  34590  2016.0    chevrolet       good  6 cylinders  gas   29499.0   \n",
       "3  35000  2019.0       toyota  excellent  6 cylinders  gas   43000.0   \n",
       "4  29990  2016.0    chevrolet       good  6 cylinders  gas   17302.0   \n",
       "\n",
       "  title_status transmission drive    type paint_color state  year_posted  \\\n",
       "0        clean    automatic   rwd   truck       black    al         2021   \n",
       "1        clean        other   4wd  pickup       black    al         2021   \n",
       "2        clean        other   4wd  pickup      silver    al         2021   \n",
       "3        clean    automatic   4wd   truck        grey    al         2021   \n",
       "4        clean        other   4wd  pickup         red    al         2021   \n",
       "\n",
       "   month_posted  weekday_posted  \n",
       "0             5               0  \n",
       "1             5               0  \n",
       "2             5               0  \n",
       "3             5               0  \n",
       "4             5               0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supprimer les lignes avec des valeurs manquantes (NaN) dans les colonnes restantes\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Dimensions après suppression des valeurs manquantes :\", df.shape)\n",
    "\n",
    "# Conversion de posting_date en objet datetime (en UTC pour gérer les fuseaux horaires si présents)\n",
    "df['posting_date'] = pd.to_datetime(df['posting_date'], errors='coerce', utc=True)\n",
    "\n",
    "# Supprimer les lignes où la conversion de date a échoué (posting_date non parseable)\n",
    "df = df[df['posting_date'].notna()]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Dimensions après nettoyage des dates :\", df.shape)\n",
    "\n",
    "# Extraire l'année, le mois et le jour de la semaine de la date de publication\n",
    "df['year_posted']   = df['posting_date'].dt.year\n",
    "df['month_posted']  = df['posting_date'].dt.month\n",
    "df['weekday_posted']= df['posting_date'].dt.weekday  # 0 = Lundi, ..., 6 = Dimanche\n",
    "\n",
    "# Supprimer la colonne originale posting_date (on a extrait ce qu'il nous faut)\n",
    "df.drop(columns=['posting_date'], inplace=True)\n",
    "\n",
    "# Filtrer les prix aberrants\n",
    "df = df[(df['price'] > 100) & (df['price'] < 250000)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Dimensions finales après filtrage des outliers de prix :\", df.shape)\n",
    "\n",
    "# Un aperçu des données après nettoyage\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À ce stade, nous avons un **DataFrame `df` propre**, contenant uniquement les **colonnes utiles** et **sans valeurs manquantes**.\r\n",
    "\r\n",
    "Les colonnes présentes sont les suivantes :\r\n",
    "\r\n",
    "- `price` (**cible à prédire**)\r\n",
    "- `year`, `manufacturer`, `condition`, `cylinders`, `fuel`, `odometer`, `transmission`, `drive`, `type`, `paint_color`, `state`, `title_status`\r\n",
    "- `year_posted`, `month_posted`, `weekday_posted` (**informations extraites de `posting_date`**)\r\n",
    "\r\n",
    "---\n",
    "Vérifions la liste finale des colonnes et quelques statistiques de base pour se faire une idée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes finales disponibles : ['price', 'year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'paint_color', 'state', 'year_posted', 'month_posted', 'weekday_posted']\n",
      "                price           year manufacturer  condition    cylinders  \\\n",
      "count   111643.000000  111643.000000       111643     111643       111643   \n",
      "unique            NaN            NaN           42          6            8   \n",
      "top               NaN            NaN         ford  excellent  6 cylinders   \n",
      "freq              NaN            NaN        20023      48961        41384   \n",
      "mean     16133.537741    2009.243813          NaN        NaN          NaN   \n",
      "std      13337.617289       9.949786          NaN        NaN          NaN   \n",
      "min        103.000000    1900.000000          NaN        NaN          NaN   \n",
      "25%       6250.000000    2006.000000          NaN        NaN          NaN   \n",
      "50%      11850.000000    2011.000000          NaN        NaN          NaN   \n",
      "75%      23590.000000    2015.000000          NaN        NaN          NaN   \n",
      "max     195000.000000    2022.000000          NaN        NaN          NaN   \n",
      "\n",
      "          fuel      odometer title_status transmission   drive    type  \\\n",
      "count   111643  1.116430e+05       111643       111643  111643  111643   \n",
      "unique       5           NaN            6            3       3      13   \n",
      "top        gas           NaN        clean    automatic     4wd   sedan   \n",
      "freq    102268           NaN       105544        91625   46823   29749   \n",
      "mean       NaN  1.119173e+05          NaN          NaN     NaN     NaN   \n",
      "std        NaN  1.932732e+05          NaN          NaN     NaN     NaN   \n",
      "min        NaN  0.000000e+00          NaN          NaN     NaN     NaN   \n",
      "25%        NaN  5.640500e+04          NaN          NaN     NaN     NaN   \n",
      "50%        NaN  1.040000e+05          NaN          NaN     NaN     NaN   \n",
      "75%        NaN  1.490000e+05          NaN          NaN     NaN     NaN   \n",
      "max        NaN  1.000000e+07          NaN          NaN     NaN     NaN   \n",
      "\n",
      "       paint_color   state  year_posted   month_posted  weekday_posted  \n",
      "count       111643  111643     111643.0  111643.000000   111643.000000  \n",
      "unique          12      51          NaN            NaN             NaN  \n",
      "top          white      ca          NaN            NaN             NaN  \n",
      "freq         26759   12273          NaN            NaN             NaN  \n",
      "mean           NaN     NaN       2021.0       4.287577        2.720350  \n",
      "std            NaN     NaN          0.0       0.452635        2.020377  \n",
      "min            NaN     NaN       2021.0       4.000000        0.000000  \n",
      "25%            NaN     NaN       2021.0       4.000000        1.000000  \n",
      "50%            NaN     NaN       2021.0       4.000000        3.000000  \n",
      "75%            NaN     NaN       2021.0       5.000000        4.000000  \n",
      "max            NaN     NaN       2021.0       5.000000        6.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"Colonnes finales disponibles :\", df.columns.tolist())\n",
    "print(df.describe(include='all'))  # statistiques rapides, inclut les numériques par défaut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(La sortie ci-dessus donne des informations statistiques : par exemple le prix moyen, le kilométrage moyen, etc., ainsi que le nombre de catégories uniques pour les colonnes non numériques si on précise include='all'.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage des variables catégorielles et normalisation des numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifions maintenant quelles sont les colonnes **catégorielles** et lesquelles sont **numériques**, afin de les traiter correctement.\r\n",
    "\r\n",
    "- **Colonnes numériques** :  \r\n",
    "  `year`, `odometer`, `year_posted`, `month_posted`, `weekday_posted`  \r\n",
    "  Ce sont des variables continues ayant une signification quantitative.\r\n",
    "\r\n",
    "- **Colonnes catégorielles** :  \r\n",
    "  `manufacturer`, `condition`, `cylinders`, `fuel`, `transmission`, `drive`, `type`, `paint_color`, `state`, `title_status`  \r\n",
    "  Ces colonnes sont typiquement de type *object* (texte) dans le DataFrame.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Nous allons appliquer un **One-Hot Encoding** sur toutes ces variables catégorielles à l’aide de `pd.get_dummies`.  \r\n",
    "Cela va créer une colonne binaire (0/1) pour chaque catégorie possible de chaque variable.  \r\n",
    "Par exemple :  \r\n",
    "→ La colonne `fuel` donnera `fuel_gas`, `fuel_diesel`, `fuel_elric`, etc.\r\n",
    "\r\n",
    "💡 **Remarque importante** :  \r\n",
    "- Nous ne ferons **pas** `drop_first=True` pour ne pas introduire de biais arbitraire (référence implicite).  \r\n",
    "- Cela entraînera une **augmentation du nombre de colonnes**, mais les réseaux de neurones peuvent le gérer.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Une fois cela fait, nous **standardiserons les variables numériques** (soustraction de la moyenne et division par l’écart-type).  \r\n",
    "Mais cette étape sera réalisée **après le split train/val/test** pour éviter toute **fuite de données**.  \r\n",
    "La normalisation se fera **uniquement** à partir des statistiques du jeu d'entraînement.\r\n",
    "\r\n",
    "Enfin, pour s’assurer que tous les datasets (train, val, test) ont les **mêmes colonnes dummies**,  \r\n",
    "nous appliquerons `get_dummies` **sur l’ensemble complet** du DataFrame dès maintenant.  \r\n",
    "Ainsi, une catégorie présente uniquement dans le test (par exemple) aura une colonne déjà prête (elle sera simplement à 0 ailleurs).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Procédons donc à l’encodage, puis à la séparation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features après encodage : 154\n",
      "Quelques-unes des colonnes encodées : ['year', 'odometer', 'year_posted', 'month_posted', 'weekday_posted', 'manufacturer_acura', 'manufacturer_alfa-romeo', 'manufacturer_aston-martin', 'manufacturer_audi', 'manufacturer_bmw', 'manufacturer_buick', 'manufacturer_cadillac', 'manufacturer_chevrolet', 'manufacturer_chrysler', 'manufacturer_datsun', 'manufacturer_dodge', 'manufacturer_ferrari', 'manufacturer_fiat', 'manufacturer_ford', 'manufacturer_gmc']\n"
     ]
    }
   ],
   "source": [
    "# Séparer la cible des features\n",
    "y = df['price'].astype('float32')  # le prix, converti en float32 (type approprié pour Keras)\n",
    "X = df.drop(columns=['price'])\n",
    "\n",
    "# Identifier les colonnes catégorielles et numériques\n",
    "cat_cols = ['manufacturer', 'condition', 'cylinders', 'fuel', \n",
    "            'transmission', 'drive', 'type', 'paint_color', \n",
    "            'state', 'title_status']\n",
    "num_cols = ['year', 'odometer', 'year_posted', 'month_posted', 'weekday_posted']\n",
    "\n",
    "# Encodage one-hot des variables catégorielles\n",
    "X_dummies = pd.get_dummies(X[cat_cols], prefix_sep='_', drop_first=False)\n",
    "# Concaténer avec les colonnes numériques non encodées\n",
    "X_numeric = X[num_cols].astype('float32')\n",
    "X_encoded = pd.concat([X_numeric, X_dummies], axis=1)\n",
    "\n",
    "print(\"Nombre de features après encodage :\", X_encoded.shape[1])\n",
    "print(\"Quelques-unes des colonnes encodées :\", X_encoded.columns[:20].tolist())  # aperçu des premières colonnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tailles : X_train = (78194, 154) | X_val = (16702, 154) | X_test = (16747, 154)\n"
     ]
    }
   ],
   "source": [
    "# Découpage en ensembles d'entraînement, validation et test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tout d'abord, séparer un ensemble de test (par ex 15% des données)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_encoded, y, test_size=0.15, random_state=42)\n",
    "# Ensuite, séparer le reste en train et validation (environ 15% validation sur le total, donc environ 0.15/0.85 ≈ 17.6% de X_temp)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "\n",
    "print(\"Tailles : X_train =\", X_train.shape, \"| X_val =\", X_val.shape, \"| X_test =\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, appliquons la normalisation (standardisation) aux colonnes numériques de nos matrices d'entrée : on ajuste uniquement sur X_train puis on transforme X_val et X_test en utilisant les paramètres calculés sur X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyennes des colonnes numériques (train) après standardisation: [0.0, 0.0, 0.0, -0.0, 0.0]\n",
      "Écarts-types des colonnes numériques (train) après standardisation: [1.0, 1.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Normalisation (StandardScaler) des features numériques\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Ajuster le scaler sur les données d'entraînement (colonnes numériques uniquement)\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "# Appliquer la transformation aux jeux de validation et de test\n",
    "X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# Vérifier que la normalisation a été effectuée (moyenne ~0, écart-type ~1 sur train)\n",
    "print(\"Moyennes des colonnes numériques (train) après standardisation:\", X_train[num_cols].mean().round(3).tolist())\n",
    "print(\"Écarts-types des colonnes numériques (train) après standardisation:\", X_train[num_cols].std().round(3).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, convertissons nos matrices de features et vecteurs cibles en formats appropriés pour Keras (numpy arrays). Pandas DataFrame peut être accepté directement, mais pour sécurité on utilisera .values pour obtenir des np.ndarray. On s'assure aussi que nos cibles y_train, y_val, y_test sont bien des vecteurs numpy de type float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format des arrays final : (78194, 154) (78194,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_np = X_train.values.astype('float32')\n",
    "X_val_np   = X_val.values.astype('float32')\n",
    "X_test_np  = X_test.values.astype('float32')\n",
    "\n",
    "y_train_np = y_train.values.astype('float32')\n",
    "y_val_np   = y_val.values.astype('float32')\n",
    "y_test_np  = y_test.values.astype('float32')\n",
    "\n",
    "print(\"Format des arrays final :\", X_train_np.shape, y_train_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos données sont prêtes. Nous avons maintenant :\n",
    "X_train_np (matrice des features d'entraînement) et y_train_np (valeurs cibles correspondantes),\n",
    "X_val_np et y_val_np,\n",
    "X_test_np et y_test_np.\n",
    "Le tout est normalisé et encodé, prêt à être ingéré par un réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du réseau de neurones pour la régression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir l'architecture de notre **réseau de neurones**.  \r\n",
    "Il s'agira d’un **réseau feedforward** simple (réseau à propagation avant), structuré comme suit :\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Structure du modèle :**\r\n",
    "\r\n",
    "- **Couche d'entrée**  \r\n",
    "  Attend un vecteur de dimension égale au nombre de variables explicatives après encodage.  \r\n",
    "  Elle sera définie implicitement via la première couche `Dense` avec `input_shape`.\r\n",
    "\r\n",
    "- **1ère couche cachée `Dense`**  \r\n",
    "  - 64 neurones  \r\n",
    "  - Activation : `ReLU`  \r\n",
    "  Ce choix permet de modéliser des relations non-linéaires sans complexifier à l’excès le modèle.  \r\n",
    "  L'activation ReLU est standard pour sa simplicité, son efficacité et l’absence de problème de vanishing gradient.\r\n",
    "\r\n",
    "- **2ème couche cachée `Dense`**  \r\n",
    "  - 32 neurones  \r\n",
    "  - Activation : `ReLU`  \r\n",
    "  Une taille plus réduite sert de **régularisation implicite**, limitant la complexité tout en gardant une bonne expressivité.\r\n",
    "\r\n",
    "- **Couche de sortie `Dense`**  \r\n",
    "  - 1 neurone  \r\n",
    "  - Activation : `linéaire`  \r\n",
    "  Appropriée pour la régression : elle permet de prédire une valeur réelle (le prix).  \r\n",
    "  On ne contraint pas explicitement à des valeurs positives, ce qui reste acceptable avec des données bien normalisées.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Fonction de perte (loss)** :  \r\n",
    "Nous utiliserons la **MSE (Mean Squared Error)**, courante en régression.  \r\n",
    "Elle est différentiable, sensible aux gros écarts, et permet au modèle de mieux gérer les erreurs importantes.  \r\n",
    "\r\n",
    "**Métrique de suivi** :  \r\n",
    "Nous suivrons également la **MAE (Mean Absolute Error)** à chaque époque.  \r\n",
    "Elle est plus facile à interpréter (valeur moyenne absolue de l’erreur en euros, par exemple).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Objectif expérimental** :  \r\n",
    "Nous allons comparer plusieurs **optimiseurs** (SGD, Adam, RMSprop, Adagrad)  \r\n",
    "et nous assurerons que **chaque modèle** commence avec les **mêmes poids initiaux**.  \r\n",
    "\r\n",
    "Pour cela :\r\n",
    "- Nous définirons une **fonction `build_model()`** qui construit un modèle neuf.\r\n",
    "- Nous utiliserons `tf.random.set_seed()` pour fixer la graine aléatoire à chaque appel, assurant une initialisation identique des poids.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Nous utiliserons **TensorFlow (Keras)** pour l’implémentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m9,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,033</span> (47.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,033\u001b[0m (47.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,033</span> (47.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,033\u001b[0m (47.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model():\n",
    "    # Fixer la graine pour avoir la même initialisation à chaque appel\n",
    "    tf.random.set_seed(42)\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(X_train_np.shape[1],)),  # Utiliser une couche Input explicite\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Tester la construction du modèle\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit dans le résumé que le réseau comporte un certain nombre de paramètres :\n",
    "Pour la couche Dense de 64 neurones en entrée, avec n features en entrée, il y a 64 * n + 64 paramètres (poids + biais).\n",
    "Pour la couche Dense de 32 neurones suivante, 32 * 64 + 32 paramètres.\n",
    "Pour la couche de sortie 1 neurone, 1 * 32 + 1 paramètres.\n",
    "Cela donne un total (affiché par model.summary()) qu'on peut noter. Maintenant, nous allons compiler le modèle avec la configuration d'apprentissage. Nous ne le faisons pas encore car nous allons le compiler différemment pour chaque optimiseur testé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle avec différents optimiseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant entraîner le **réseau défini précédemment** avec plusieurs **algorithmes d'optimisation**, puis **comparer leurs performances**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### Optimiseurs comparés :\r\n",
    "\r\n",
    "- **SGD (Stochastic Gradient Descent)**  \r\n",
    "  Descente de gradient stochastique classique.  \r\n",
    "  Utilisé ici **sans momentum**, avec un learning rate de **0.01**.  \r\n",
    "  Sert de **référence \"brute\"** face aux méthodes adaptatives.\r\n",
    "\r\n",
    "- **Adam (Adaptive Moment Estimation)**  \r\n",
    "  Méthode adaptative très populaire.  \r\n",
    "  Combine moyenne des gradients et moyenne des gradients au carré.  \r\n",
    "  Learning rate typique : **0.001**.  \r\n",
    "  Reconnue pour une **convergence rapide en début d’entraînement**.\r\n",
    "\r\n",
    "- **RMSprop**  \r\n",
    "  Méthode adaptative qui maintient une moyenne glissante des **gradients au carré**.  \r\n",
    "  Learning rate : **0.001**.  \r\n",
    "  Efficace pour les **réseaux profonds** ou les gradients instables.\r\n",
    "\r\n",
    "- **Adagrad**  \r\n",
    "  Méthode adaptative plus ancienne.  \r\n",
    "  Accumule les gradients pour **réduire progressivement le learning rate**.  \r\n",
    "  Learning rate par défaut : **0.001**.  \r\n",
    "  Rapide au début mais peut **ralentir fortement sur le long terme**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "> Nous nous concentrerons sur **ces quatre méthodes**.  \r\n",
    "> D’autres variantes (Adadelta, Adamax, SGD avec momentum...) peuvent être testées, mais ne seront pas analysées ici.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### Pour chaque optimiseur, nous suivrons le même protocole :\r\n",
    "\r\n",
    "1. **Construction d’un nouveau modèle**  \r\n",
    "   → Grâce à `build_model()` avec **poids initiaux identiques** (graine fixée).\r\n",
    "\r\n",
    "2. **Compilation**  \r\n",
    "   → Optimiseur choisi + perte **MSE** + métrique **MAE**.\r\n",
    "\r\n",
    "3. **Entraînement**  \r\n",
    "   → Sur **100 époques** (valeur identique pour tous).  \r\n",
    "   → Suivi de la **MAE de validation** (`validation_data=(X_val_np, y_val_np)`).\r\n",
    "\r\n",
    "4. **Mesure du temps total d'entraînement**.\r\n",
    "\r\n",
    "5. **Enregistrement de l’historique**  \r\n",
    "   → Pour comparer les **courbes de perte et MAE**.\r\n",
    "\r\n",
    "6. **Évaluation finale**  \r\n",
    "   → Sur le **jeu de test** : pour vérifier la **généralisation** du modèle.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Après chaque entraînement, nous commenterons :\r\n",
    "- **La vitesse de convergence** (via la courbe de validation MAE)\r\n",
    "- **La performance finale** (val/test MAE)\r\n",
    "\r\n",
    "ℹ️ L’entraînement sur l’ensemble des données peut **prendre plusieurs minutes**, selon la machine et la complexité du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec l'optimiseur SGD (Descente de Gradient Stochastique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par l'optimiseur SGD classique. Nous nous attendons à une convergence plus lente, car SGD utilise un taux d'apprentissage fixe (0.001 par défaut) et n'adapte pas les pas de gradient individuellement. Sans momentum, SGD peut nécessiter de nombreuses époques pour approcher le minimum. Observons comment la MAE diminue au fil des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Résultats - Optimiseur SGD\n",
      "Durée d'entraînement : 435.3 secondes\n",
      "Train : MAE = 3646.99 (22.7%), RMSE = 60.39 (0.4%)\n",
      "Val   : MAE = 3795.98 (23.6%), RMSE = 61.61 (0.4%)\n",
      "Test  : MAE = 3675.78 (22.9%), RMSE = 60.63 (0.4%)\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle avec SGD\n",
    "import time\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "model_sgd = build_model()\n",
    "optimizer_sgd = keras.optimizers.SGD(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "model_sgd.compile(optimizer=optimizer_sgd, loss='mae', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train_np, y_train_np,\n",
    "    epochs=100, batch_size=32,\n",
    "    validation_data=(X_val_np, y_val_np),\n",
    "    verbose=0\n",
    ")\n",
    "elapsed_sgd = time.time() - start_time\n",
    "\n",
    "# Évaluation\n",
    "train_mse, train_mae = model_sgd.evaluate(X_train_np, y_train_np, verbose=0)\n",
    "val_mse, val_mae = model_sgd.evaluate(X_val_np, y_val_np, verbose=0)\n",
    "test_mse, test_mae = model_sgd.evaluate(X_test_np, y_test_np, verbose=0)\n",
    "\n",
    "# RMSE\n",
    "train_rmse = sqrt(train_mse)\n",
    "val_rmse = sqrt(val_mse)\n",
    "test_rmse = sqrt(test_mse)\n",
    "\n",
    "# Moyenne du prix réel (pour avoir des %)\n",
    "mean_price = np.mean(y_test_np)\n",
    "\n",
    "# Affichage clair et interprétable\n",
    "print(\"\\n📊 Résultats - Optimiseur SGD\")\n",
    "print(f\"Durée d'entraînement : {elapsed_sgd:.1f} secondes\")\n",
    "print(f\"Train : MAE = {train_mae:.2f} ({train_mae / mean_price * 100:.1f}%), RMSE = {train_rmse:.2f} ({train_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Val   : MAE = {val_mae:.2f} ({val_mae / mean_price * 100:.1f}%), RMSE = {val_rmse:.2f} ({val_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Test  : MAE = {test_mae:.2f} ({test_mae / mean_price * 100:.1f}%), RMSE = {test_rmse:.2f} ({test_rmse / mean_price * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétation SGD : Avec SGD, la descente est progressive. Sur les 100 époques, on s'attend à voir la MAE de validation diminuer lentement. Le résultat imprimé donne la MAE finale : on peut constater par exemple que la MAE entraînement est plus basse que la MAE validation, signe d'un léger sur-apprentissage ou simplement que le modèle n'a pas encore atteint son minimum global sur val. Le temps d'entraînement est notre référence de base (par exemple X secondes). Globalement, SGD semble nécessiter davantage d'époques pour atteindre une erreur faible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec l'optimiseur Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînons maintenant le même modèle avec Adam, qui devrait converger plus rapidement grâce à l'adaptation du taux de learning pour chaque paramètre. On utilise les paramètres par défaut (lr=0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Entraînement avec Adam \n",
    "model_adam = build_model()\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_adam.compile(optimizer=optimizer_adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history_adam = model_adam.fit(X_train_np, y_train_np, epochs=100, batch_size=32, \n",
    "                               validation_data=(X_val_np, y_val_np), verbose=0)\n",
    "elapsed_adam = time.time() - start_time\n",
    "\n",
    "# Évaluation\n",
    "train_mse, train_mae = model_adam.evaluate(X_train_np, y_train_np, verbose=0)\n",
    "val_mse, val_mae = model_adam.evaluate(X_val_np, y_val_np, verbose=0)\n",
    "test_mse, test_mae = model_adam.evaluate(X_test_np, y_test_np, verbose=0)\n",
    "baseline_mae = test_mae  # Pour les comparaisons futures\n",
    "\n",
    "# RMSE\n",
    "train_rmse = sqrt(train_mse)\n",
    "val_rmse = sqrt(val_mse)\n",
    "test_rmse = sqrt(test_mse)\n",
    "\n",
    "print(\"\\nRésultats - Optimiseur Adam\")\n",
    "print(f\"Durée d'entraînement : {elapsed_adam:.1f} secondes\")\n",
    "print(f\"Train : MAE = {train_mae:.2f} ({train_mae / mean_price * 100:.1f}%), RMSE = {train_rmse:.2f} ({train_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Val   : MAE = {val_mae:.2f} ({val_mae / mean_price * 100:.1f}%), RMSE = {val_rmse:.2f} ({val_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Test  : MAE = {test_mae:.2f} ({test_mae / mean_price * 100:.1f}%), RMSE = {test_rmse:.2f} ({test_rmse / mean_price * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétation Adam : Comme attendu, Adam converge beaucoup plus vite. La MAE de validation baisse significativement plus rapidement dans les premières époques comparé à SGD. Au bout de 100 époques, Adam atteint généralement une MAE plus faible que SGD. Par exemple, on peut voir que la MAE validation est inférieure à celle obtenue avec SGD (et assez proche de la MAE train, signe que le modèle généralise bien). Le temps d'exécution total n'est pas énormément plus élevé que pour SGD (Adam calcule plus de choses par itération, mais ici la différence reste faible). Adam se distingue donc par son efficacité à réduire l'erreur rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons maintenant RMSprop, un optimiseur adaptatif proche d'Adam (il n'utilise pas la composante de momentum du gradient moyen, seulement le carré moyen). On s'attend à une performance de convergence également rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement avec RMSprop\n",
    "model_rms = build_model()\n",
    "optimizer_rms = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model_rms.compile(optimizer=optimizer_rms, loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history_rms = model_rms.fit(X_train_np, y_train_np, epochs=100, batch_size=32, \n",
    "                             validation_data=(X_val_np, y_val_np), verbose=0)\n",
    "elapsed_rms = time.time() - start_time\n",
    "\n",
    "# Évaluation\n",
    "train_mse, train_mae = model_rms.evaluate(X_train_np, y_train_np, verbose=0)\n",
    "val_mse, val_mae = model_rms.evaluate(X_val_np, y_val_np, verbose=0)\n",
    "test_mse, test_mae = model_rms.evaluate(X_test_np, y_test_np, verbose=0)\n",
    "\n",
    "train_rmse = sqrt(train_mse)\n",
    "val_rmse = sqrt(val_mse)\n",
    "test_rmse = sqrt(test_mse)\n",
    "\n",
    "print(\"\\n📊 Résultats - Optimiseur RMSprop\")\n",
    "print(f\"Durée d'entraînement : {elapsed_rms:.1f} secondes\")\n",
    "print(f\"Train : MAE = {train_mae:.2f} ({train_mae / mean_price * 100:.1f}%), RMSE = {train_rmse:.2f} ({train_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Val   : MAE = {val_mae:.2f} ({val_mae / mean_price * 100:.1f}%), RMSE = {val_rmse:.2f} ({val_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Test  : MAE = {test_mae:.2f} ({test_mae / mean_price * 100:.1f}%), RMSE = {test_rmse:.2f} ({test_rmse / mean_price * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétation RMSprop : RMSprop affiche une convergence similaire à Adam dans ce cas. Souvent, on observe que RMSprop atteint presque les mêmes performances qu'Adam en validation, peut-être avec une courbe un peu moins lisse. La MAE finale validation et test sont comparables à celles d'Adam (parfois légèrement supérieures ou inférieures selon les cas, mais de même ordre). Le temps d'entraînement est du même ordre de grandeur. En résumé, RMSprop est très efficace sur ce problème, proche d'Adam en termes de vitesse de convergence et de performance atteinte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, testons Adagrad. C'est un optimiseur adaptatif qui diminue progressivement le learning rate effectif à mesure que les gradients s'accumulent. Il peut être très rapide initialement mais a tendance à stagner sur le long terme une fois qu'il a beaucoup réduit ses pas de gradient.\n",
    "python\n",
    "Copier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement avec Adagrad\n",
    "model_ada = build_model()\n",
    "optimizer_ada = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "model_ada.compile(optimizer=optimizer_ada, loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history_ada = model_ada.fit(X_train_np, y_train_np, epochs=100, batch_size=32, \n",
    "                             validation_data=(X_val_np, y_val_np), verbose=0)\n",
    "elapsed_ada = time.time() - start_time\n",
    "\n",
    "# Évaluation\n",
    "train_mse, train_mae = model_ada.evaluate(X_train_np, y_train_np, verbose=0)\n",
    "val_mse, val_mae = model_ada.evaluate(X_val_np, y_val_np, verbose=0)\n",
    "test_mse, test_mae = model_ada.evaluate(X_test_np, y_test_np, verbose=0)\n",
    "\n",
    "train_rmse = sqrt(train_mse)\n",
    "val_rmse = sqrt(val_mse)\n",
    "test_rmse = sqrt(test_mse)\n",
    "\n",
    "print(\"\\n📊 Résultats - Optimiseur Adagrad\")\n",
    "print(f\"Durée d'entraînement : {elapsed_ada:.1f} secondes\")\n",
    "print(f\"Train : MAE = {train_mae:.2f} ({train_mae / mean_price * 100:.1f}%), RMSE = {train_rmse:.2f} ({train_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Val   : MAE = {val_mae:.2f} ({val_mae / mean_price * 100:.1f}%), RMSE = {val_rmse:.2f} ({val_rmse / mean_price * 100:.1f}%)\")\n",
    "print(f\"Test  : MAE = {test_mae:.2f} ({test_mae / mean_price * 100:.1f}%), RMSE = {test_rmse:.2f} ({test_rmse / mean_price * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Interprétation Adagrad : On observe généralement qu'Adagrad commence bien (premières époques efficaces) mais qu'il atteint assez vite un palier. En 100 époques, il est possible que la MAE de validation d'Adagrad soit un peu supérieure à celle d'Adam/RMSprop, signe qu'il a cessé d'apprendre de manière agressive. La MAE entraînement peut rester un peu plus élevée que pour les autres (signe que l'optimiseur a ralenti), et la MAE validation/test suit le même trend. Le temps d'entraînement est similaire, la différence résidant dans la progression de l'erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison globale des optimiseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que chaque optimiseur a été testé, comparons visuellement leurs courbes de convergence (MAE et loss MSE au fil des époques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Courbes de MAE de validation pour chaque optimiseur\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history_sgd.history['val_mae'], label='SGD')\n",
    "plt.plot(history_adam.history['val_mae'], label='Adam')\n",
    "plt.plot(history_rms.history['val_mae'], label='RMSprop')\n",
    "plt.plot(history_ada.history['val_mae'], label='Adagrad')\n",
    "plt.title(\"MAE de validation vs époques (par optimiseur)\")\n",
    "plt.xlabel(\"Époques\")\n",
    "plt.ylabel(\"MAE (validation)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Courbes de loss (MSE) de validation pour chaque optimiseur\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history_sgd.history['val_loss'], label='SGD')\n",
    "plt.plot(history_adam.history['val_loss'], label='Adam')\n",
    "plt.plot(history_rms.history['val_loss'], label='RMSprop')\n",
    "plt.plot(history_ada.history['val_loss'], label='Adagrad')\n",
    "plt.title(\"Loss (MSE) de validation vs époques (par optimiseur)\")\n",
    "plt.xlabel(\"Époques\")\n",
    "plt.ylabel(\"MSE (validation)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sur la première figure (MAE validation), on voit nettement que **Adam** et **RMSprop** descendent beaucoup plus rapidement que **SGD** dans les premières époques.*\r\n",
    "\r\n",
    "- Adam atteint une faible MAE dès environ **20 à 30 époques**, puis se stabilise.\r\n",
    "- La courbe de SGD baisse plus lentement et **n’a pas encore atteint le niveau d’Adam à la 100ᵉ époque**.\r\n",
    "- RMSprop suit une trajectoire proche d’Adam, légèrement en retrait vers la fin.\r\n",
    "- Adagrad baisse vite au début, parfois presque aussi vite qu’Adam, mais **se stabilise plus haut**, ce qui est cohérent avec la nature de son taux d’apprentissage qui diminue avec le temps.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "*La deuxième figure (loss MSE) montre un comportement similaire :*\r\n",
    "\r\n",
    "- Adam et RMSprop **diminuent fortement la loss** initialement, puis stagnent.\r\n",
    "- SGD baisse de façon **plus linéaire**, sans atteindre le niveau des autres.\r\n",
    "- Adagrad diminue rapidement sa loss puis la courbe devient quasiment plate en fin d'entraînement.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "# Performances finales\r\n",
    "\r\n",
    "*En résumé :*\r\n",
    "\r\n",
    "- **Adam** et **RMSprop** atteignent les **meilleures MAE de validation**.\r\n",
    "- **Adagrad** est légèrement en dessous.\r\n",
    "- **SGD** est **le moins performant à 100 époques**, mais il continue de s’améliorer régulièrement.\r\n",
    "\r\n",
    "*Sur le jeu de test*, les MAE sont proches de celles obtenues en validation, ce qui signifie que **le modèle généralise bien**. Il n’y a **pas de sur-apprentissage visible**, en partie grâce à la **taille importante du dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évolution des MAE entraînement/validation par optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history_sgd.history['mae'], label='SGD - Train', linestyle='-')\n",
    "plt.plot(history_sgd.history['val_mae'], label='SGD - Val', linestyle='--')\n",
    "\n",
    "plt.plot(history_adam.history['mae'], label='Adam - Train', linestyle='-')\n",
    "plt.plot(history_adam.history['val_mae'], label='Adam - Val', linestyle='--')\n",
    "\n",
    "plt.plot(history_rms.history['mae'], label='RMSprop - Train', linestyle='-')\n",
    "plt.plot(history_rms.history['val_mae'], label='RMSprop - Val', linestyle='--')\n",
    "\n",
    "plt.plot(history_ada.history['mae'], label='Adagrad - Train', linestyle='-')\n",
    "plt.plot(history_ada.history['val_mae'], label='Adagrad - Val', linestyle='--')\n",
    "\n",
    "plt.title(\"MAE - Entraînement vs Validation pour chaque optimiseur\")\n",
    "plt.xlabel(\"Époques\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des erreurs sur les données de test (prédictions - réalité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for name, model in {\n",
    "    \"SGD\": model_sgd,\n",
    "    \"Adam\": model_adam,\n",
    "    \"RMSprop\": model_rms,\n",
    "    \"Adagrad\": model_ada\n",
    "}.items():\n",
    "    y_pred = model.predict(X_test_np).flatten()\n",
    "    errors = y_test_np.flatten() - y_pred\n",
    "    sns.kdeplot(errors, label=name)\n",
    "\n",
    "plt.title(\"Distribution des erreurs de prédiction (test)\")\n",
    "plt.xlabel(\"Erreur (réelle - prédite)\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédictions vs Valeurs réelles (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "for name, model in {\n",
    "    \"SGD\": model_sgd,\n",
    "    \"Adam\": model_adam,\n",
    "    \"RMSprop\": model_rms,\n",
    "    \"Adagrad\": model_ada\n",
    "}.items():\n",
    "    y_pred = model.predict(X_test_np).flatten()\n",
    "    plt.scatter(y_test_np, y_pred, alpha=0.3, label=name)\n",
    "\n",
    "min_val = y_test_np.min()\n",
    "max_val = y_test_np.max()\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', label=\"Réel = Prédit\")\n",
    "\n",
    "plt.title(\"Prédictions vs Valeurs réelles (jeu de test)\")\n",
    "plt.xlabel(\"Valeurs réelles\")\n",
    "plt.ylabel(\"Prédictions\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'importance des caractéristiques par ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant **évaluer l'importance de chaque feature** dans la prédiction du prix.\r\n",
    "\r\n",
    "La méthode employée est une **analyse par ablation** :  \r\n",
    "on retire **une colonne à la fois** du jeu de features et on réentraîne le modèle (en utilisant l'optimiseur **Adam**, qui a donné les meilleurs résultats) pour observer **l'impact sur la MAE**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Principe :**\r\n",
    "\r\n",
    "- Si la suppression d'une feature **provoque une forte augmentation** de la MAE finale, cela signifie que cette feature était **importante**.\r\n",
    "- Si la suppression **n'impacte que très peu** la MAE, c’est que la feature était **peu informative** ou **redondante\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "⚠️ **Remarque importante** :  \r\n",
    "Cette méthode est **coûteuse en temps de calcul**, car elle nécessite :\r\n",
    "- D'entraîner **un nouveau modèle pour chaque feature supprimée**.\r\n",
    "- Avec 14 features d’entrée, cela fait **14 entraînements complets** de 100 époques chacun.\r\n",
    "\r\n",
    "*Dans un contexte réel, on pourrait :*\r\n",
    "- **Réduire le nombre d’époques**,\r\n",
    "- Ou utiliser des techniques plus rapides comme :  \r\n",
    "  - **l’importance permutationnelle**,  \r\n",
    "  - ou **les valeurs SHAP (SHapley Additive exPlanations)**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Ici, nous procédons à cette approche complète à but **démonstratif**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['year','manufacturer','condition','cylinders','fuel','odometer',\n",
    "            'transmission','drive','type','paint_color','state','title_status',\n",
    "            'year_posted','month_posted','weekday_posted']\n",
    "\n",
    "feature_mae = {}  # dictionnaire pour stocker la MAE test sans chaque feature\n",
    "\n",
    "for feat in features:\n",
    "    # Préparer les données d'entraînement/val/test sans la feature 'feat'\n",
    "    if feat in num_cols:\n",
    "        # Retirer la colonne numérique directement\n",
    "        X_train_drop = X_train.drop(columns=[feat])\n",
    "        X_val_drop = X_val.drop(columns=[feat])\n",
    "        X_test_drop = X_test.drop(columns=[feat])\n",
    "    else:\n",
    "        # Retirer toutes les colonnes dummies correspondant à la feature catégorielle\n",
    "        cols_to_drop = [col for col in X_train.columns if col.startswith(feat + \"_\")]\n",
    "        X_train_drop = X_train.drop(columns=cols_to_drop)\n",
    "        X_val_drop = X_val.drop(columns=cols_to_drop)\n",
    "        X_test_drop = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Convertir explicitement en numpy array de type float32\n",
    "    X_train_drop_np = X_train_drop.values.astype('float32')\n",
    "    X_val_drop_np   = X_val_drop.values.astype('float32')\n",
    "    X_test_drop_np  = X_test_drop.values.astype('float32')\n",
    "\n",
    "    # Entraîner un modèle Adam sur ces données modifiées\n",
    "    model_feat = keras.Sequential([\n",
    "        keras.Input(shape=(X_train_drop_np.shape[1],)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model_feat.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse', metrics=['mae'])\n",
    "    model_feat.fit(X_train_drop_np, y_train_np, epochs=100, batch_size=32,\n",
    "                   validation_data=(X_val_drop_np, y_val_np), verbose=0)\n",
    "\n",
    "    # Évaluer la MAE sur le jeu de test\n",
    "    _, mae_no_feat = model_feat.evaluate(X_test_drop_np, y_test_np, verbose=0)\n",
    "    feature_mae[feat] = mae_no_feat\n",
    "    print(f\"Sans '{feat}' -> MAE test: {mae_no_feat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois ce code exécuté, nous avons dans feature_mae la MAE (sur test) du modèle entraîné sans chaque feature. Rappelons que le modèle complet (avec toutes features) avait une MAE de base d'environ {baseline_mae:.2f} sur le test. Analysons l'augmentation d'erreur induite par l'absence de chaque colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'augmentation de MAE par rapport au modèle complet\n",
    "baseline = baseline_mae\n",
    "mae_increase = {feat: feature_mae[feat] - baseline for feat in feature_mae}\n",
    "# Trier les features par impact décroissant\n",
    "sorted_impacts = sorted(mae_increase.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Augmentation de MAE en retirant chaque feature :\")\n",
    "for feat, inc in sorted_impacts:\n",
    "    print(f\"{feat:15} +{inc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des impacts sous forme de barres horizontales\n",
    "feats = [f for f, inc in sorted_impacts]\n",
    "incs = [inc for f, inc in sorted_impacts]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(feats, incs, color='skyblue')\n",
    "plt.xlabel(\"Augmentation de la MAE (sans la feature)\")\n",
    "plt.ylabel(\"Feature retirée\")\n",
    "plt.title(\"Impact de la suppression d'une feature sur l'erreur de prédiction\")\n",
    "plt.gca().invert_yaxis()  # mettre la plus impactante en haut\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interprétation des résultats d'ablation\n",
    "\n",
    "L’analyse par ablation permet d’évaluer l’importance relative de chaque feature. Voici les principales observations :\n",
    "\n",
    "---\n",
    "\n",
    "## Features les plus déterminantes :\n",
    "\n",
    "- **year (année du véhicule)** :  \n",
    "  La suppression de cette colonne entraîne une nette dégradation de la MAE. C’est cohérent avec le fait que l’âge du véhicule est un facteur déterminant de sa valeur.\n",
    "\n",
    "- **odometer (kilométrage)** :  \n",
    "  L’impact est également très marqué. Plus une voiture a roulé, plus sa valeur tend à diminuer. Le modèle utilise donc fortement cette information.\n",
    "\n",
    "- **manufacturer (marque)** :  \n",
    "  L’absence de cette variable empêche le modèle de distinguer des marques aux positionnements différents (ex. BMW vs Honda), ce qui affecte directement la qualité des prédictions.\n",
    "\n",
    "- **condition** et **title_status** :  \n",
    "  Ces deux variables apportent des informations critiques sur l’état et l’historique du véhicule (ex. voiture accidentée ou remise à neuf), expliquant leur rôle important.\n",
    "\n",
    "- **type (catégorie du véhicule)** :  \n",
    "  Son retrait a également un effet significatif. Certains types (camionnettes, SUV, sportives) sont associés à des gammes de prix bien distinctes.\n",
    "\n",
    "---\n",
    "\n",
    "## Features avec impact faible ou négligeable :\n",
    "\n",
    "- **paint_color (couleur)** :  \n",
    "  Très faible influence. La couleur n'affecte que marginalement le prix de vente, sauf cas très particuliers.\n",
    "\n",
    "- **state (État géographique)** :  \n",
    "  La suppression de cette colonne n’entraîne qu’une variation minime de la MAE. Les différences régionales de prix sont peu marquées dans ce dataset.\n",
    "\n",
    "- **year_posted, month_posted, weekday_posted** :  \n",
    "  Ces variables liées à la date de publication ne semblent pas utiles pour la prédiction du prix. Elles peuvent refléter des effets saisonniers ou de marché, mais dans ce cas précis, leur impact est négligeable.\n",
    "\n",
    "---\n",
    "\n",
    "## Résumé :\n",
    "\n",
    "Les variables **mécaniques et intrinsèques** du véhicule (année, kilométrage, état, marque, statut, type) sont celles qui influencent le plus fortement le prix.  \n",
    "Les variables **cosmétiques ou contextuelles** (couleur, date de publication, localisation) ont un effet très limité.\n",
    "\n",
    "Cela confirme l’intuition du marché automobile : ce sont les caractéristiques physiques et techniques du véhicule qui déterminent sa valeur perçue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous avons construit un modèle de régression pour estimer le prix de véhicules d'occasion à partir de données issues d'annonces Craigslist.\r\n",
    "\r\n",
    "Après un travail de préparation des données (nettoyage, encodage des variables catégorielles, normalisation), nous avons entraîné un réseau de neurones simple et comparé l'effet de différents algorithmes d'optimisation sur la convergence du modèle.\r\n",
    "\r\n",
    "Les résultats ont montré que :\r\n",
    "\r\n",
    "- Adam et RMSprop convergent plus rapidement que SGD, atteignant une erreur (MAE) plus faible en moins d’époques.\r\n",
    "- Adagrad converge rapidement au début mais stagne ensuite.\r\n",
    "- SGD est plus lent sans réglage particulier, mais pourrait atteindre des performances similaires avec plus d’époques ou un learning rate mieux adapté.\r\n",
    "\r\n",
    "Ces observations confirment que les optimiseurs adaptatifs sont souvent plus efficaces pour des modèles de ce type, en particulier Adam, qui s’avère être un bon choix par défaut.\r\n",
    "\r\n",
    "L’analyse par ablation a mis en évidence que les variables les plus importantes pour prédire le prix sont l’année du véhicule, le kilométrage, l’état général, le statut du titre et la marque. Ces éléments sont effectivement ceux que les acheteurs considèrent en priorité.\r\n",
    "\r\n",
    "Les variables comme la couleur, la date de publication ou l’État géographique ont peu d’influence et peuvent être ignorées sans perte significative de performance.\r\n",
    "\r\n",
    "En conclusion, nous avons obtenu un modèle atteignant une MAE d’environ **{baseline_mae:.2f} €** sur le jeu de test, ce qui représente une erreur moyenne raisonnable compte tenu du prix typique des véhicules.\r\n",
    "\r\n",
    "Des pistes d’amélioration possibles incluent :\r\n",
    "\r\n",
    "- Intégrer le modèle exact du véhicule via des techniques d’encodage plus avancées (embedding).\r\n",
    "- Ajuster l’architecture du réseau (nombre de couches, de neurones).\r\n",
    "- Utiliser des techniques de régularisation (dropout, weight decay).\r\n",
    "- Comparer avec d’autres familles de modèles (arbres de décision, forêts aléatoires, XGBoost...).\r\n",
    "\r\n",
    "Globalement, ce projet montre qu’un réseau de neurones simple, correctement entraîné avec un optimiseur comme Adam, permet d’obtenir des résultats solides et cohérents avec les connaissances du domaine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
